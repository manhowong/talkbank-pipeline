{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb152186-996f-4333-88dc-568856819a05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TalkBank Data Pipeline\n",
    "\n",
    "Man Ho Wong  \n",
    "University of Pittsburgh\n",
    "\n",
    "Here are the major steps of the pipeline:\n",
    "1. Get download URLs for target collection(s)\n",
    "2. Screen for potential datasets in the target collection(s)\n",
    "3. Download potential datasets to local drive\n",
    "4. Search for target files in potential datasets and index them\n",
    "5. Standardize header labels\n",
    "6. Add participant identifier (optional)\n",
    "7. Save the cleaned index table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2dc480-1a5e-4fbd-ad60-0986ca9b062d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e588f08-14db-4304-8146-07f5f2a93b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See README.md for installation of the following packages\n",
    "from bs4 import BeautifulSoup\n",
    "from etc.pittchat import get_age_m as get_age_m # get age in months\n",
    "from etc.pittchat import get_age_d as get_age_d # get age in days\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pylangacq                                # read CHAT files\n",
    "import requests\n",
    "from tqdm import tqdm                           # progress bar (optional)\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# To get all labels of a given field (e.g. 'group')\n",
    "def get_labels(var):\n",
    "    labels_by_corpus = {}\n",
    "    corpus_set = set(data_idx.corpus)\n",
    "    for c in corpus_set:\n",
    "        labels_by_corpus[c] = set(data_idx[var][data_idx.corpus==c])\n",
    "    return labels_by_corpus\n",
    "\n",
    "# Pretty printing for better readability:\n",
    "# - print dict in compact format instead of one item per line\n",
    "# - Items will be in alphabetical order; Counter in descending order\n",
    "# - Nested Dict will be printed with suitable indentation\n",
    "import pprint  # pretty printing\n",
    "cp = pprint.PrettyPrinter(compact=True, sort_dicts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43e8b2-0690-4bcd-8a67-84cd2fa86cbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data from last run (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2425ab8-3d5d-460c-9a20-16b393f1b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = pickle.load(open('data/paths.pkl', 'rb'))\n",
    "data_idx = pickle.load(open('data/data_idx_raw.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eeeb7-fbcf-419e-b9dc-2371a2deb959",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Get download URLs for target collection(s)\n",
    "\n",
    "- The following code gets the download URLs for the zip files of the corpora in the target collection(s).  \n",
    "- You can specify a target collection by the name of the collection (e.g. \"childes\") and the name of the folder (e.g. \"Eng-NA\").\n",
    "\n",
    "> To find the name of a specific collection or folder:\n",
    "> 1. Go to the [TalkBank Browser](https://sla.talkbank.org/TBB).\n",
    "> 2. Select the collection in the pull-down menu and navigate to the folder you are interested in.\n",
    "> 3. The name of the collection and/or folder is in the directory path under the pull-down menu.\n",
    "\n",
    "***Note:** This code can only find URLs for public collections. For password-protected collections, please follow the instructions in TalkBank's documentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72aa59-5dd6-484f-bb63-057631d36318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get download URLs for zip files\n",
    "\n",
    "############################## User Input ######################################\n",
    "# 1. Enter the name of the TalkBank collection below\n",
    "bank_name = \"   \"\n",
    "# 2. Enter target folder(s) in the collection, e.g. folders=['Eng-NA','Eng-UK']\n",
    "#    To include all downloadable corpora in the data search, \n",
    "#    enter an empty string, i.e. folders = ['']\n",
    "#    Case-sensitive!\n",
    "folders = ['   ']\n",
    "################################################################################\n",
    "\n",
    "# Find all URLs in the selected collection\n",
    "root_url = 'https://' + bank_name + '.talkbank.org/data/'\n",
    "# List of urls (only root_url for now)\n",
    "urls = [root_url]\n",
    "print('Looking for URLs in {}...' .format(bank_name))\n",
    "for url in urls:\n",
    "    if not url.endswith('.zip'): # inspect current url if not a zip file\n",
    "        # get urls under current url:\n",
    "        req = requests.get(url)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        for a in soup.find_all('a'):\n",
    "            path = a.get('href')\n",
    "            # add urls under current url to 'urls':\n",
    "            if not( ('?' in path) or path.startswith('/')): # exclude query code and root folder\n",
    "                urls.append(url + path)\n",
    "\n",
    "# Get URLs for zip files\n",
    "zip_urls = [url for url in urls for d in folders \n",
    "            if d in url and url.endswith('.zip')]\n",
    "print(\"Done! All URLs for zip files are stored in 'zip_urls'. \\n\"\n",
    "      \"There are {} downloadable corpora. Here is an example: \\n{}\"\n",
    "      .format(len(zip_urls), zip_urls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35528a23-3fad-48b6-9996-548758cf7bd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Screen for potential datasets in the target collection(s) \n",
    "\n",
    "## Step 2A (To skip, go to Step 2B)\n",
    "- The following code screens for potential datasets that contain the target file(s) you need.\n",
    "\n",
    "- **Strategies:** This step is optional, but it is recommended for efficiency: It narrows down the scope of data search later in this pipeline if your target collection is large. You don't need to use very stringent screening criteria here, but rather something more general but good enough to narrow down the scope of the data search. The goal is to find the datasets containing at least one file which satisfies *some* of your data requirements. A more detailed data search will be done in the latter step.\n",
    "\n",
    "- This step uses the `pylangacq` package to look up the metadata in file headers. Please see the package's documentation for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd190cf-5b1f-418c-8659-e5fd9486a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of corpora matching the criteria\n",
    "search_result = []\n",
    "# Inspect every corpus on the target_urls list:\n",
    "for url in tqdm(zip_urls):  # tqdm for progress bar\n",
    "    corpus = pylangacq.read_chat(url) # read current corpus into a Reader\n",
    "    for h in corpus.headers():\n",
    "        ########################### User Input #################################        \n",
    "        # Insert if-conditions for search criteria below:\n",
    "        if (        ):\n",
    "        ########################################################################\n",
    "            search_result.append(url)  # add corpus to 'search_result'\n",
    "            break  # move on to the next corpus once a file matches the criteria\n",
    "\n",
    "print('\\n{} corpora matching the criteria:'.format(len(search_result)))\n",
    "# Create a dataframe to store corpus info\n",
    "corpus_names = [url.split('/')[-1].replace('.zip','') for url in search_result]\n",
    "homepages    = [url.replace('data','access').replace('zip','html') for url in search_result]\n",
    "local_paths  = ['data/'+bank_name+'/'+url.lstrip(root_url).rstrip('.zip') for url in search_result]\n",
    "paths = pd.DataFrame({'corpus':corpus_names,'homepage':homepages,\n",
    "                      'zip_url':search_result,'local_path':local_paths})\n",
    "paths\n",
    "\n",
    "# Save 'paths'\n",
    "with open('data/paths.pkl', 'wb') as f:\n",
    "    pickle.dump(paths, f, -1)\n",
    "print(\"'paths' was saved as 'data/paths.pkl'. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab64de8-b836-47d5-999e-c74df14d375c",
   "metadata": {},
   "source": [
    "## Step 2B (Run this if Step 2A was skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1f38b-b760-4d53-8866-ba0e4555ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following lines if Step 2A was skipped\n",
    "\n",
    "# corpus_names = [url.split('/')[-1].replace('.zip','') for url in search_result]\n",
    "# homepages    = [url.replace('data','access').replace('zip','html') for url in search_result]\n",
    "# local_paths  = ['data/'+bank_name+'/'+url.lstrip(root_url).rstrip('.zip') for url in search_result]\n",
    "# paths = pd.DataFrame({'corpus':corpus_names,'homepage':homepages,\n",
    "#                       'zip_url':search_result,'local_path':local_paths})\n",
    "# with open('data/paths.pkl', 'wb') as f:\n",
    "#     pickle.dump(paths, f, -1)\n",
    "# print(\"'paths' was saved as 'data/paths.pkl'. \")\n",
    "# path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca96944-cc80-40b6-b11b-33ecda667d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Download potential datasets to local drive\n",
    "\n",
    "The following code downloads and extracts the potential datasets to your local drive. A folder named \"data\" will be created in the current directory to store the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a05be9-b061-4500-8b89-aa8d1a6ecea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in paths['zip_url']:    \n",
    "    print('Downloading and extracting {}...'.format(url))\n",
    "    # create directories\n",
    "    fname = url.split(\"/\")[-1]    \n",
    "    dest_dir = 'data/' + bank_name + '/' + url.lstrip(root_url).rstrip(fname)\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    # Download corpus from URL\n",
    "    zip_path = dest_dir + fname\n",
    "    urllib.request.urlretrieve(url, zip_path)    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(dest_dir)\n",
    "    os.remove(zip_path)\n",
    "       \n",
    "print(\"Done! Zip files were downloaded and extracted to 'data/'. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bf697-f538-4b24-95f1-f75b63b68334",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Search for target files in potential datasets and index them\n",
    "\n",
    "- The following code searches for files that satisfy your data requirements by checking the metadata in their file headers. An index table will be generated to store the metadata of these files for easier file lookup later. Each row of the index table is a file (i.e. an entry) and each column is a file header field. You can name each column however you prefer.\n",
    "\n",
    "> File header fields are listed in the [CHAT manual](https://talkbank.org/manuals/CHAT.html). Note that not all files contain the same header fields. To check what header fields are available in a file, you may view the header of the file in the TalkBank Browser or download the file and view it on your computer.\n",
    "\n",
    "- This step uses the `pylangacq` package to look up the metadata in file headers. Please see the package's documentation for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95609680-aad6-40a0-ad82-1c3809685e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## User Input ######################################\n",
    "# Create an index table by building an empty dataframe with desired column names\n",
    "data_idx = pd.DataFrame(columns=[                   ])\n",
    "################################################################################\n",
    "\n",
    "# Search for target files and store their metadata to 'data_idx'\n",
    "for corpus_dir in tqdm(paths['local_path']):       # tqdm for progress bar  \n",
    "    corpus = pylangacq.Reader.from_dir(corpus_dir) # Read corpus into a Reader\n",
    "    for f in corpus:                               # Loop through each file\n",
    "        h = f.headers()[0]                         # Header of each file\n",
    "        ############################ User Input ################################\n",
    "        # Insert below the conditions to skip files you don't need:\n",
    "        # e.g. skip files without 'CHI' in the participant field of the header:\n",
    "        # if 'CHI' not in h['Participants']: continue\n",
    "        \n",
    "        # Create a dict with field names SAME as column names in data_idx\n",
    "        # Number of fields must be equal to the number of columns in data_idx!\n",
    "        # Insert below the code to retrieve file/ header info in each field\n",
    "        # Info can be retrieved from the header (h) or the file (f)\n",
    "        # Tips:\n",
    "        # - Age: Age info in the header is in (y,m,d) format. Use get_age_d or\n",
    "        #   get_age_m to convert ymd to age in days or months\n",
    "        # - File paths: replace '/' in file paths with '\\' for Windows\n",
    "        # - Handling missing values:\n",
    "        #   Some files may have missing fields in the header, use if-condition\n",
    "        #   to spot missing values and replace them with nan or empty string ''\n",
    "        \n",
    "        info_dict = {    }\n",
    "        ########################################################################\n",
    "        # Fill the next row of data_idx with info_dict\n",
    "        data_idx.loc[len(data_idx)] = info_dict\n",
    "                    \n",
    "# Replace empty string with 'unspecified'\n",
    "data_idx.replace(to_replace = '', value = 'unspecified', inplace=True)\n",
    "data_idx.head()\n",
    "\n",
    "# Save unprocessed 'data_idx' as 'data/data_idx_raw.pkl'\n",
    "with open('data/data_idx_raw.pkl', 'wb') as f:\n",
    "    pickle.dump(data_idx, f, -1)\n",
    "    \n",
    "print(\"'data_idx' was saved as 'data/data_idx_raw.pkl'. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66170c4-f7d3-4cf3-880b-cf8da1256f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# 5. Standardize header labels\n",
    "\n",
    "Different studies may use different texts (i.e. \"header labels\" in this pipeline) to specify the same recording conditions in the file header (e.g. either `TD` or `typical` might be used to label files from typically developed children). Besides, since metadata might be entered manually, human errors such as typos or missing data could be found. To index files of the same conditions from different studies correctly, we need to standardize the header labels from all the studies. \n",
    "\n",
    "Here is the general workflow:\n",
    "1. Inspect header labels\n",
    "2. Check study design or header label definition\n",
    "3. Integrate header labels in the index table\n",
    "4. Replace missing labels\n",
    "\n",
    "From now on, the pipeline will only work on the index table instead of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d6221-a49a-4083-9200-da60d6f2bde5",
   "metadata": {},
   "source": [
    "## 5.1 Inspect header labels\n",
    "To get all the labels used for a specific header field (i.e. column) in the index table `data_idx`, you can call this function (defined at the beginning of this noteboook): `get_labels( <column name> )`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363597c3-3165-4f0f-9577-03aef5f0bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## User Input ######################################\n",
    "# Enter the column name of data_idx you want to inspect\n",
    "cp.pprint( get_labels('') )           # pprint for compact printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05163b8f-4838-475e-840c-ed18438a0484",
   "metadata": {},
   "source": [
    "## 5.2 Check study design / header label definition\n",
    "\n",
    "Please visit the homepages of the datasets for more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81b4de-2464-438f-aaf8-682d304fe681",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['homepage'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba4632-db88-461b-a6cc-e10c266bcbf6",
   "metadata": {},
   "source": [
    "## 5.3 Integrate header labels in the index\n",
    "\n",
    "The goal of this step is to standardize header labels so that all files are indexed with a common set of labels for every header field.\n",
    "\n",
    "## 5.4 Replace missing labels\n",
    "\n",
    "Missing labels will be filled according to the documentation of the corpus from where a file is downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3bc6c-269a-4dbe-a353-c42715eaf35e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Save the cleaned index table\n",
    "\n",
    "You may use the index table, `data_idx`, for retrieving data in your dataset. Save the table for data analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a34e50-5d03-4f7a-ad05-6a36d12f7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned 'data_idx' as 'data/data_idx_cleaned.pkl'\n",
    "with open('data/data_idx_cleaned.pkl', 'wb') as f:\n",
    "    pickle.dump(data_idx, f, -1)    \n",
    "print(\"Cleaned 'data_idx' was saved as 'data/data_idx_cleaned.pkl'. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f82c4-99a6-4ce4-a2f8-8d677f116f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to read data into readers and export as pickle\n",
    "\n",
    "# corpus_readers = [pylangacq.Reader.from_dir(d) for d in tqdm(paths['local_path'])]\n",
    "# with open('data/corpus_readers.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus_readers, f, -1)    \n",
    "# print(\"'corpus_readers' was saved as 'data/corpus_readers.pkl'. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "09b147ebe07113da5e9b813fa92f74f688f5ea88363e0a460d1e837983bd8e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
