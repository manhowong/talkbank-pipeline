{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1e5b4a-16eb-4ef8-adec-90f881548609",
   "metadata": {},
   "source": [
    "# TalkBank Data Pipeline Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e588f08-14db-4304-8146-07f5f2a93b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See README.md for installation of the following packages\n",
    "from bs4 import BeautifulSoup\n",
    "from etc.pittchat import get_age_m as get_age_m # get age in months\n",
    "from etc.pittchat import get_age_d as get_age_d # get age in days\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pylangacq                                # read CHAT files\n",
    "import requests\n",
    "from tqdm import tqdm                           # progress bar (optional)\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# To get all labels of a given field (e.g. 'group')\n",
    "def get_labels(var):\n",
    "    labels_by_corpus = {}\n",
    "    corpus_set = set(data_idx.corpus)\n",
    "    for c in corpus_set:\n",
    "        labels_by_corpus[c] = set(data_idx[var][data_idx.corpus==c])\n",
    "    return labels_by_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eeeb7-fbcf-419e-b9dc-2371a2deb959",
   "metadata": {},
   "source": [
    "# 1. Get download URLs for target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce72aa59-5dd6-484f-bb63-057631d36318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for URLs in childes...\n",
      "Done! All URLs for zip files are stored in 'zip_urls'. \n",
      "There are 48 downloadable corpora. Here is an example: \n",
      "https://childes.talkbank.org/data/Eng-NA/Bates.zip\n"
     ]
    }
   ],
   "source": [
    "# Get download URLs for zip files\n",
    "\n",
    "############################## User Input ######################################\n",
    "# 1. Enter the name of the TalkBank collection below\n",
    "bank_name = \"childes\"\n",
    "# 2. Enter target folder(s) in the collection, e.g. folders=['Eng-NA','Eng-UK']\n",
    "#    To include all downloadable corpora in the data search, \n",
    "#    enter an empty string, i.e. folders = ['']\n",
    "#    Case-sensitive!\n",
    "folders = ['Eng-NA']\n",
    "################################################################################\n",
    "\n",
    "# Find all URLs in the selected collection\n",
    "root_url = 'https://' + bank_name + '.talkbank.org/data/'\n",
    "# List of urls (only root_url for now)\n",
    "urls = [root_url]\n",
    "print('Looking for URLs in {}...' .format(bank_name))\n",
    "for url in urls:\n",
    "    if not url.endswith('.zip'): # inspect current url if not a zip file\n",
    "        # get urls under current url:\n",
    "        req = requests.get(url)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        for a in soup.find_all('a'):\n",
    "            path = a.get('href')\n",
    "            # add urls under current url to 'urls':\n",
    "            if not( ('?' in path) or path.startswith('/')): # exclude query code and root folder\n",
    "                urls.append(url + path)\n",
    "\n",
    "# Get URLs for zip files\n",
    "zip_urls = [url for url in urls for d in folders \n",
    "            if d in url and url.endswith('.zip')]\n",
    "print(\"Done! All URLs for zip files are stored in 'zip_urls'. \\n\"\n",
    "      \"There are {} downloadable corpora. Here is an example: \\n{}\"\n",
    "      .format(len(zip_urls), zip_urls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35528a23-3fad-48b6-9996-548758cf7bd3",
   "metadata": {},
   "source": [
    "# 2. Screen for interested datasets\n",
    "\n",
    "# Search criteria:  \n",
    "    # - Child ('CHI') is included as participant.\n",
    "    # - Info about Child's SES or mother's SES/education is provided.\n",
    "    # Note: Check if 'MOT' is present before checking SES/education info\n",
    "    #       (if the condition before 'and' is False, the second condition won't\n",
    "    #       be evaluated. This is not only more efficient, but also prevents\n",
    "    #       error when evaluating the second condition if 'MOT' is absent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd190cf-5b1f-418c-8659-e5fd9486a403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▏                                                                             | 3/48 [00:07<01:41,  2.25s/it]"
     ]
    }
   ],
   "source": [
    "# list of corpora matching the criteria\n",
    "search_result = []\n",
    "# Inspect every corpus on the target_urls list:\n",
    "for url in tqdm(zip_urls):  # tqdm for progress bar\n",
    "    corpus = pylangacq.read_chat(url) # read current corpus into a Reader\n",
    "    for h in corpus.headers():\n",
    "        ########################### User Input #################################        \n",
    "        # Insert if-conditions for search criteria below:\n",
    "        if (\n",
    "            # 'CHI' must be included\n",
    "            ('CHI' in h['Participants'])             \n",
    "            and            \n",
    "            # Must contain info about either SES or mother's education\n",
    "            (   # Child's or mother's SES \n",
    "                ((h['Participants']['CHI']['ses'] != '') or\n",
    "                (('MOT' in h['Participants']) and (h['Participants']['MOT']['ses'] != '')))\n",
    "                or                \n",
    "                # Mother's education\n",
    "                (('MOT' in h['Participants']) and (h['Participants']['MOT']['education'] != ''))\n",
    "            )            \n",
    "        ):\n",
    "        ########################################################################\n",
    "            search_result.append(url)  # add corpus to 'search_result'\n",
    "            break  # move on to the next corpus once a file matches the criteria\n",
    "\n",
    "print('\\n{} corpora matching the criteria:'.format(len(search_result)))\n",
    "# Create a dataframe to store corpus info\n",
    "corpus_names = [url.split('/')[-1].replace('.zip','') for url in search_result]\n",
    "homepages    = [url.replace('data','access').replace('zip','html') for url in search_result]\n",
    "local_paths  = ['data/'+bank_name+'/'+url.lstrip(root_url).rstrip('.zip') for url in search_result]\n",
    "paths = pd.DataFrame({'corpus':corpus_names,'homepage':homepages,'zip_url':search_result})\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca96944-cc80-40b6-b11b-33ecda667d27",
   "metadata": {},
   "source": [
    "# 3. Download datasets to local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a05be9-b061-4500-8b89-aa8d1a6ecea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_paths = []\n",
    "\n",
    "for url in paths['zip_url']:    \n",
    "    print('Downloading and extracting {}...'.format(url))\n",
    "    # create directories\n",
    "    fname = url.split(\"/\")[-1]    \n",
    "    dest_dir = 'data/' + bank_name + '/' + url.lstrip(root_url).rstrip(fname)\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    # Download corpus from URL\n",
    "    zip_path = dest_dir + fname\n",
    "    urllib.request.urlretrieve(url, zip_path)    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(dest_dir)\n",
    "    os.remove(zip_path)\n",
    "    local_paths.append(zip_path.rstrip('.zip'))  # directory of extracted files\n",
    "\n",
    "# Add local paths of corpora to dataframe 'paths'\n",
    "paths['local_path'] = local_paths\n",
    "        \n",
    "print(\"Done! Zip files were downloaded and extracted to 'data/'. \")\n",
    "\n",
    "# Save 'paths'\n",
    "with open('data/paths.pkl', 'wb') as f:\n",
    "    pickle.dump(paths, f, -1)\n",
    "print(\"'paths' was saved as 'data/paths.pkl'. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bf697-f538-4b24-95f1-f75b63b68334",
   "metadata": {},
   "source": [
    "# 4. Search for target files in interested datasets and index them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95609680-aad6-40a0-ad82-1c3809685e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## User Input ######################################\n",
    "# Create an index table by building an empty dataframe with desired column names\n",
    "data_idx = pd.DataFrame(\n",
    "           columns=['file_path',\n",
    "                    'corpus',\n",
    "                    'year',        # recording year\n",
    "                    'participants',# participants of recording\n",
    "                    'name',        # Child's name\n",
    "                    'age_d',       # Child's age in days\n",
    "                    'age_m',       # Child's age in months\n",
    "                    'sex',         # Child's sex\n",
    "                    'group',       # Child's group (e.g. typically developed)\n",
    "                    'ses',         # Child's SES (same as mother's)\n",
    "                    'mot_edu',     # Mother's education\n",
    "                    'situation',   # Recording situation (e.g. play session)\n",
    "                    'activities',  # Activities during recording (e.g. Toy play)\n",
    "                    'study_type'   # type of study (e.g. longitudinal study)\n",
    "                   ])\n",
    "################################################################################\n",
    "\n",
    "# Get meta data of files\n",
    "for corpus_dir in tqdm(paths['local_path']):             # tqdm for progress bar  \n",
    "    corpus = pylangacq.Reader.from_dir(corpus_dir) # Read corpus into a Reader\n",
    "    for f in corpus:                               # Loop through each file\n",
    "        h = f.headers()[0]                         # Header of each file\n",
    "        ############################ User Input ################################\n",
    "        # Insert below the conditions to skip files you don't need:\n",
    "        #   e.g. skip files without 'CHI' meta info and children > 72 months  \n",
    "        if 'CHI' not in h['Participants'] or get_age_m(f.ages()[0])>72: continue\n",
    "        # Create a dict with field names SAME as column names in data_idx\n",
    "        # Number of fields must be equal to number of columns in data_idx!\n",
    "        # Insert below the code to retrieve file/ header info in each field\n",
    "        # Info can be retrieve from the header (h) or the file (f)\n",
    "        info_dict = {\n",
    "                    'corpus':       h['Participants']['CHI']['corpus'],\n",
    "                    'sex':          h['Participants']['CHI']['sex'],\n",
    "                    'name':         h['Participants']['CHI']['name'],\n",
    "                    'group':        h['Participants']['CHI']['group'],                    \n",
    "                    'ses':          h['Participants']['CHI']['ses'],\n",
    "                    'participants': f.participants(),\n",
    "                    # Age info in header is in (y,m,d) format. Use get_age_d\n",
    "                    # or get_age_m to convert ymd to age in days or months\n",
    "                    'age_d':        get_age_d(f.ages()[0]),\n",
    "                    'age_m':        get_age_m(f.ages()[0]),\n",
    "                    # replace '/' in file paths with '\\' for Windows\n",
    "                    'file_path':    f.file_paths()[0].replace('\\\\','/'),\n",
    "                    # Handling missing fields and missing values:\n",
    "                    #   Some files may have missing fields in the header,\n",
    "                    #   use if-condition to spot missing fields and fill in\n",
    "                    #   missing values with e.g. nan or empty string ''\n",
    "                    'year':  list(h['Date'])[0].year if 'Date' in h else np.nan,\n",
    "                    'situation':    h['Situation'] if 'Situation' in h else '',\n",
    "                    'activities':   h['Activities'] if 'Activities' in h else '',\n",
    "                    'study_type':   h['Types'] if 'Types' in h else '',\n",
    "                    'mot_edu':      h['Participants']['MOT']['education']\n",
    "                                     if 'MOT' in h['Participants'] else ''\n",
    "                    }\n",
    "        # some missing values can be replaced with values from another field\n",
    "        #   e.g. use mother's SES as child's SES if available\n",
    "        if h['Participants']['CHI']['ses']=='' and 'MOT' in h['Participants']:\n",
    "            info_dict['ses'] = h['Participants']['MOT']['ses']\n",
    "        ########################################################################\n",
    "        # Fill the next row of data_idx with info_dict\n",
    "        data_idx.loc[len(data_idx)] = info_dict\n",
    "                    \n",
    "# Replace empty string with 'unspecified'\n",
    "data_idx.replace(to_replace = '', value = 'unspecified', inplace=True)\n",
    "data_idx.head()\n",
    "\n",
    "# Save unprocessed 'data_idx' as 'data/data_idx_raw.pkl'\n",
    "with open('data/data_idx_raw.pkl', 'wb') as f:\n",
    "    pickle.dump(data_idx, f, -1)\n",
    "    \n",
    "print(\"'data_idx' was saved as 'data/data_idx_raw.pkl'. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a243f5a-db11-4e2a-ac52-3bb6dc7867ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Data integration and cleaning\n",
    "\n",
    "Corpora from different studies may use different terms to label the same recording conditions and participant information in the header. For example, both `TD` and `typical` are used to label files from typically developed children in different studies. Moreover, since header information is entered manually, human errors such as typos or missing data might be found. To integrate the data from different studies, we need to standardize the labels used to specify recording conditions and participant information. This step ensures that the files in our integrated dataset will be indexed correctly.\n",
    "\n",
    "## 5.1 Inspect header labels and check definitions\n",
    "To get all the labels used for a specific header field (e.g. `group`), you can call this function (defined in the header of this noteboook): `get_labels( <header field> )`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363597c3-3165-4f0f-9577-03aef5f0bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.pprint( get_labels('group') ) # pprint for compact printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05163b8f-4838-475e-840c-ed18438a0484",
   "metadata": {},
   "source": [
    "To check the definitions of these labels, you can visit the dataset's website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81b4de-2464-438f-aaf8-682d304fe681",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['homepage'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba4632-db88-461b-a6cc-e10c266bcbf6",
   "metadata": {},
   "source": [
    "## 5.2 Standardize header labels and fill in missing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fb987-71a9-4440-8cdc-2f1f452c902e",
   "metadata": {},
   "source": [
    "## 5.3 Example\n",
    "\n",
    "### `group` labels\n",
    "\n",
    "As shown above, the `group` information of is not properly specified in every corpus: \n",
    "- According the documentations of the corpora, all children in the dataset should be typically developed (`TD` in `group`). Therefore, we will change the `unspecified` label to `TD`.\n",
    "- in the Gleason corpus, labels `normal` and `typical` are used in addition to `TD`. We will change all of this labels to `TD`.\n",
    "- In the Hall corpus, the labels for `group` and `ses` were switched in some entries (e.g. `White,UC` should be a label for `ses`). We will switch them first and change all `group` labels to `TD`.\n",
    "- In the VanHouten corpus, all children are `TD`, but it also contains entries with adolescent mothers. Assuming that we only want to study children with adult mothers, we will remove the entries with adolescent mothers.\n",
    "\n",
    "Let's take a quick look at the annotation mistakes in the Hall corpus first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceaf0c5-6dd8-40d1-8a20-47196d05259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['TD', 'typical', 'normal']\n",
    "data_idx[['group','ses','study_type']][(data_idx.corpus=='Hall') & \n",
    "                                       ~(data_idx.group.isin(labels))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70092425-740f-45f6-a794-e6f7c28d842f",
   "metadata": {},
   "source": [
    "In row 1482 and 1485, the label `White,UC` (i.e. upper class white) was entered in the field `group`, though it is clearly a label for `ses`. We will copy the labels to `ses` of the corresponding rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c293ac5-f679-4ff7-ab8d-ed321756b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idx.loc[data_idx.group=='White,UC','ses'] = 'White,UC'\n",
    "\n",
    "# Print to check if ses labels were updated\n",
    "data_idx.loc[[1482,1485],['group','ses','study_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d26900-d63f-4ac7-a517-41ab39c3158d",
   "metadata": {},
   "source": [
    "Note that the `group` labels are still `White,UC`, but I will change them to `TD` together with other entries later.\n",
    "Next, we will drop the entries with adolescent mothers in the VanHouten corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef976c80-ebf4-46e4-a984-ce4760299e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_tags = ['MOT_Adolescent', 'MOT_Adolescent_', 'MOT_adolescent']\n",
    "n_drop_group = len(data_idx[data_idx.group.isin(drop_tags)])\n",
    "data_idx.drop(data_idx[data_idx.group.isin(drop_tags)].index, inplace=True)\n",
    "\n",
    "# Check current labels\n",
    "cp.pprint(get_labels('group'))\n",
    "\n",
    "print(n_drop_group, 'entries dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162b8f0-2069-4989-a3ee-3e1ae7eb3eee",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "Finally, we will change all labels to `TD` at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d335bd-580f-4e57-9a6a-959f32d6af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all 'group' label to TD\n",
    "data_idx['group'] = 'TD'\n",
    "\n",
    "# Check updated labels\n",
    "get_labels('group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5beb3-d416-4410-a8f4-c0ec9de173f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `mot_edu` labels\n",
    "\n",
    "First, let's find out all the labels used for `mot_edu` in every corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c3a10-57aa-4051-89c9-ec6bec05103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.pprint( get_labels('mot_edu') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902de6c-081d-4801-a064-caccb9370192",
   "metadata": {},
   "source": [
    "--\n",
    "  \n",
    "Different classifications of education level are used in three corpora (HSLLD, NewmanRatner and VanHouten). HSLLD has a very detailed classification while VanHouten has only three classes:\n",
    "- The labels used in HSLLD correspond to the school grade year the mother achieved. 'GED' is equivalent to completing grade 12 in the U.S. educational system. The labels '13' and beyond correspond to some post-secondary education (e.g. vocational training).\n",
    "- According to VanHouten's documentation, mother’s education is classified as MOT_1 (completed junior high), MOT_2 (completed high school) or MOT_3 (some post-secondary education).\n",
    "\n",
    "To standardize all the labels, we can use a less detailed classification without losing information we need for our study.\n",
    "We will merge the current labels into the following labels according to their common definitions: \n",
    "\n",
    "- `JH-`, `HS-`, `HS`, `HS+` for:\n",
    "    - before completing junior high (before grade 9),\n",
    "    - before completing high school (before GED or grade 12),\n",
    "    - completed high school (GED or grade 12), and\n",
    "    - some post-secondary education (after high school).\n",
    "\n",
    "- `AD`, `UG-`, `UG` for:\n",
    "    - associate's degree,\n",
    "    - some undergraduate education, and\n",
    "    - bachelor's degree.\n",
    "\n",
    "- `MS`, `JD`, `DR` for:\n",
    "    - master's degree,\n",
    "    - juris doctor degree, and\n",
    "    - doctoral degree.\n",
    "    \n",
    "We can map the current `mot_edu` labels to the above new labels with a `dictionary` and update `data_idx` with the new labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec592cd-74ce-4c6a-bc1c-e5976329a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update 'mot_edu'\n",
    "def update_mot_edu(data):\n",
    "    label = data\n",
    "    #' mot_edu' label mapping\n",
    "    EDU_DICT = {\n",
    "              'JH-':['6','7','8'],\n",
    "              'HS-':['9','10','11','11+','almost 12','MOT_1'],\n",
    "              'HS':['10 , GED','11 , GED','12','12+',\n",
    "                    'High_School_Diploma','MOT_2'],\n",
    "              'HS+':['13','13+','14','15','16','MOT_3'],\n",
    "              'AD':[\"Associate's_Degree\"],\n",
    "              'UG-':['Some_College'],\n",
    "              'UG':['College'], \n",
    "              'MS':[\"College_Master's\"],\n",
    "              'JD':['College_J.D.'],    \n",
    "              'DR':['College_Doctoral'],\n",
    "              'unspecified':['**','102','XX','unspecified']\n",
    "               }\n",
    "    \n",
    "    for key in EDU_DICT:\n",
    "        if data in EDU_DICT[key]:\n",
    "            label = key\n",
    "    return label\n",
    "\n",
    "# Update 'mot_edu'\n",
    "data_idx.mot_edu = data_idx.mot_edu.map(update_mot_edu)\n",
    "\n",
    "# Check if update was successful\n",
    "cp.pprint(get_labels('mot_edu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7e346-9cde-45bb-a226-0d6a6cece6c3",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "### `ses` labels\n",
    "\n",
    "Below are the labels for `ses`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef912a-c915-4d63-b21c-2536922eb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_labels('ses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca52457-2172-41ea-b707-a4aedfab8a89",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "In these corpora, `WC`, `MC`, `UC` and `LI` mean 'working class', 'middle class', 'upper class' and 'low income' respectively. Different labels are used in different corpora:\n",
    "- In the Hall corpus, each class of `WC` and `UC` is subdivided into two racial groups, `Black` and `White`. If the primary goal is to investigate the effects of SES and mother's education on vocabulary development, we can merge the labels for different racial groups but of the same SES class together if the sample distribution among different racial groups is balanced.\n",
    "- In the Hicks corpus, `LI` ('low income') is used instead of common SES class such as `WC`. Since the majority of low-come families presumably belong to the working class, we can change the label `LI` to `WC`. - Besides, the children in the subdirectories '1st', '2nd', and '5th' were considered coming from `MC` families. The children in the subdirectory 'del' were from lower class (presumably `WC`) families. We can assign the missing labels accordingly.\n",
    "- It is stated on the homepage of the Bernstein corpus that \"the mothers were all college-educated women, who were native-born Americans with white-collar husband\". Since all families in the study have similar background and some of the files were labeled `MC`, it is very likely that other files should also be labeled `MC`.\n",
    "- The Brown corpus contains data from three children: Adam (`MC`), Sarah (`WC`) and Eve (`unspecified`). We can assign the missing labels accordingly.\n",
    "- All children in the Demetras2 corpus were from `WC` families.\n",
    "- All children in the Gleason corpus were from `MC` families.\n",
    "- All children in the HSLLD corpus were from low-income families. We will use the label `WC`.\n",
    "- There is only one child in the Nelson corpus and she came from a `MC` family.\n",
    "\n",
    "There are two corpora where no SES information could be found:\n",
    "- NewmanRatner corpus\n",
    "- VanHouten corpus: Only Hollingshead index (a measurement of SES) is provided.\n",
    "\n",
    "Let's begin cleaning up the `ses` labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf4686-d671-464f-8755-00f160628ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge 'ses' labels\n",
    "def merge_ses(data):\n",
    "    label = data\n",
    "    # 'ses' label mapping\n",
    "    SES_DICT = {'WC':['WC', 'Black,WC', 'White,WC', 'LI'],\n",
    "                'UC':['UC', 'Black,UC', 'White,UC']}\n",
    "    for key in SES_DICT:\n",
    "        if data in SES_DICT[key]:\n",
    "            label = key\n",
    "    return label\n",
    "\n",
    "# merge 'ses' labels in Hall and Hicks corpus: \n",
    "data_idx.ses = data_idx.ses.map(merge_ses)\n",
    "\n",
    "# Update labels in Hicks corpus\n",
    "for folder in ['1st','2nd','5th']:\n",
    "    data_idx.loc[((data_idx.corpus=='Hicks') & \n",
    "                  (data_idx.file_path.map(lambda x: folder in x))),'ses'] = 'MC'\n",
    "data_idx.loc[((data_idx.corpus=='Hicks') & \n",
    "              (data_idx.file_path.map(lambda x: 'del' in x))),'ses'] = 'WC'\n",
    "\n",
    "# Update labels in Brown corpus\n",
    "data_idx.loc[((data_idx.corpus=='Brown')&(data_idx.name=='Adam')),'ses']  = 'MC'\n",
    "data_idx.loc[((data_idx.corpus=='Brown')&(data_idx.name=='Sarah')),'ses'] = 'WC'\n",
    "\n",
    "# Update labels in other corpora\n",
    "data_idx.loc[data_idx.corpus=='Bernstein','ses'] = 'MC'\n",
    "data_idx.loc[data_idx.corpus=='Demetras2','ses'] = 'WC'\n",
    "data_idx.loc[data_idx.corpus=='Gleason','ses']   = 'MC'\n",
    "data_idx.loc[data_idx.corpus=='HSLLD','ses']     = 'WC'\n",
    "data_idx.loc[data_idx.corpus=='Nelson','ses']    = 'MC'\n",
    "\n",
    "# # Check updated labels\n",
    "cp.pprint(get_labels('ses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc6fdd-ceb8-4e3a-b7c9-568d910c397e",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "### `situation` labels\n",
    "\n",
    "The labels for different situations (e.g. play session, book reading, etc.) where the recordings were made could get very detailed in some files. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65466445-4084-4d45-9cf9-3f72f0ddb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_idx.situation.sample(10, random_state=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab60cb-866e-494e-b02e-ccaf0a22052a",
   "metadata": {},
   "source": [
    "To clean up the data and remove the files that we don't need, it is not necessary to read each of the `situation` labels one by one. We can just look for the keywords we need in these labels.\n",
    "\n",
    "For example, we can remove the files that were recorded in less naturalistic situation (e.g. book reading and elicited data where the discourse was more or less 'planned') to get a fair comparison between different corpora. To do this, we will remove the entries which contain the keywords \"reading\" and \"elicit\" in their `situation` field. Note that sometimes such information may be stored in `activities` or `study_type`. Therefore, we will look for the keywords in *all* of these three fields.\n",
    "\n",
    "Let's get the most common words used in the labels of these fields (less common words are likely irrelvant to the recording conditions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b8a1d-a47f-4408-a599-56455c34f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to count keywords in labels\n",
    "def count_keywords(var):  # 'var': the variable which the labels specify\n",
    "    keywords = []            \n",
    "    for f in data_idx[var]:\n",
    "        keywords.extend(f.lower().split())  # get a list of keywords\n",
    "    return Counter(keywords)\n",
    "\n",
    "print('\\nTop 20 situation keywords (word, count):')\n",
    "cp.pprint(count_keywords('situation').most_common(50))\n",
    "\n",
    "print('\\nTop 20 activity keywords (word, count):')\n",
    "cp.pprint(count_keywords('activities').most_common(50))\n",
    "\n",
    "print('\\nAll study type keywords (word, count):') #study type is not as detailed\n",
    "cp.pprint(count_keywords('study_type').most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab9f75-d3ea-4121-9b79-9dab575e75c4",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "From the lists of keywords, we can see that there are a few situations or activities with less natural child-mother interaction, such as book reading, elicited tasks and explanatory tasks (e.g. magnet task in the HSLLD corpus). Therefore, we can drop the entries with labels containing the following keywords:  \n",
    "`read, book, story, elicit, explanatory, magnet`\n",
    "\n",
    "In addition, some files contain maternal interviews where the contents are adult speech instead of child-directed speech. Therefore, we will also drop the entries with labels containing the keyword `interview` too. Besides `situation`, `activities`, and `study_type`, we will also need to look for the keywords in `file_path` as some of these keywords may only appear in the file path (e.g. folder name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e76c7-024c-47a1-85c2-0744eafa8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files with labels containing these words will be dropped:\n",
    "#   (in regular expression format)\n",
    "drop_keys = 'read|book|story|elicit|explanatory|magnet|interview'  \n",
    "\n",
    "# Select files based and 'drop_keys' and get a list of indices\n",
    "#  (keywords are case-insentive)\n",
    "drop_index = data_idx[data_idx.situation.str.contains(drop_keys, case=False) |\n",
    "                       data_idx.activities.str.contains(drop_keys, case=False) |\n",
    "                       data_idx.study_type.str.contains(drop_keys, case=False) |\n",
    "                       data_idx.file_path.str.contains(drop_keys, case=False)\n",
    "                      ].index\n",
    "\n",
    "# Drop the selected files\n",
    "data_idx.drop(drop_index, inplace=True)\n",
    "\n",
    "# Check keywords again after dropping\n",
    "print('\\nTop 20 situation keywords (word, count):')\n",
    "cp.pprint(count_keywords('situation').most_common(50))\n",
    "\n",
    "print('\\nTop 20 activity keywords (word, count):')\n",
    "cp.pprint(count_keywords('activities').most_common(50))\n",
    "\n",
    "print('\\nAll study type keywords (word, count):') #study type is not as detailed\n",
    "cp.pprint(count_keywords('study_type').most_common())\n",
    "\n",
    "print('\\n{} entries were dropped!' .format(len(drop_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff20946-b90d-4f0e-89cc-b4c6c9241767",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Add participant identifier (optional)\n",
    "\n",
    "## Example: Matching files by participants\n",
    "\n",
    "You may have noticed from the field `study_type` that some files are labeled with `cross` or `long`. These labels refer to cross-sectional or longitudinal studies. Longitudinal studies are studies where repeated observations are made with the same participants over a period of time. In our dataset, some recordings were made with the same child participants during their development. Therefore, a child participant may have multiple files in a corpus.\n",
    "\n",
    "We may identify all files associated with a child participant by matching the values in the `name` field (name of child participant). However, files with the same child name are not necessarily associated with the same child. For example, \"Adam\" can be found in the Brown and VanHouten corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066906c3-d495-47f4-bb7b-56c2383a7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpora containing files with child named 'Adam'\n",
    "set(data_idx[data_idx.name=='Adam'].corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde3f09-8920-49e9-b89e-0f4b93990528",
   "metadata": {},
   "source": [
    "Therefore, we can't simply identify the files by `name`. Instead, we can combine the corpus name and the child name, and use the combined name as the child's unique identifier (`child_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d15184-2ce7-437f-a02a-b14f429e9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join corpus name and child name\n",
    "child_id = data_idx[['corpus', 'name']].agg('_'.join, axis=1)\n",
    "\n",
    "# Insert a new column for 'child_id' as the 6th column\n",
    "data_idx.insert(5, 'child_id', child_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c2437-b806-44b8-82b6-d8cd13755bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example\n",
    "print(\"Example: Child ID for 'Adam' in different corpora\")\n",
    "display(data_idx[data_idx.name=='Adam'].head(2))\n",
    "display(data_idx[data_idx.name=='Adam'].tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800c76f-fdab-46d1-a660-2c2016483a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for plotting\n",
    "# Count files by 'sex', 'ses', 'mot_edu' or 'group'; count dropped files\n",
    "sex_count = data_idx.groupby('sex').aggregate('count').file_path\n",
    "ses_count = data_idx.groupby('ses').aggregate('count').file_path\n",
    "mot_edu_count = data_idx.groupby('mot_edu').aggregate('count').file_path\n",
    "group_count = data_idx.groupby('group').aggregate('count').file_path\n",
    "total_count = pd.Series({'Remaining files':len(data_idx),\n",
    "                         'Dropped files':len(drop_index)+n_drop_group})\n",
    "\n",
    "# ==============================================================================\n",
    "# Create a figure object\n",
    "\n",
    "# Subplot dimension ratio\n",
    "gs_kw = dict(width_ratios=[1,1,1], height_ratios=[1,4,0.2])\n",
    "\n",
    "# Create a figure with 6 axes (i.e. 6 subplots), namely A, B, C, D, E, F\n",
    "fig, ax = plt.subplot_mosaic([['A', 'A', 'B'],  # A spans across two grids\n",
    "                              ['C', 'D', 'E'],\n",
    "                              ['F', 'F', 'F']], # F spans across three grids                            \n",
    "                              gridspec_kw=gs_kw, figsize=(10,14),\n",
    "                              constrained_layout=True)\n",
    "cmap = plt.cm.YlGn  # color map\n",
    "accent_color = cmap(0.7)\n",
    "\n",
    "# ==============================================================================\n",
    "# Plotting\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot A\n",
    "ax['A'].hist(data_idx.age_m, 72, histtype='step', color=accent_color, lw=1.3)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot B to E\n",
    "\n",
    "def plot_stack(ax_idx, data, colors, thres=0):\n",
    "    data = data.sort_values(ascending=False)  # sort by count\n",
    "    data_pc = data/sum(data)*100  # get percentage from count\n",
    "    curr_sum = 0  # percentage sum (required for stacking bars)\n",
    "    for i, pc in enumerate(data_pc):\n",
    "        ax[ax_idx].bar('x', pc, label=data_pc.index[i], bottom=curr_sum,\n",
    "                       color=colors[i], edgecolor='w', linewidth=1)\n",
    "        curr_sum = curr_sum + pc  # for stacking bars  \n",
    "        # make label\n",
    "        if data.index[i] == '':\n",
    "            label = \"Unspecified ({})\".format(data[i]).upper()\n",
    "        elif pc >= thres:\n",
    "            label = \"{} ({})\".format(data.index[i], data[i]).upper()\n",
    "        else: label = ''\n",
    "        label_ypos = curr_sum - (pc/2)\n",
    "        ax[ax_idx].text(0, label_ypos, label, ha=\"center\", va=\"center\")\n",
    "\n",
    "for k, v in {'B':sex_count, 'C':ses_count, \n",
    "             'D':mot_edu_count, 'E':group_count}.items():\n",
    "    if k == 'B':\n",
    "        thres = 0\n",
    "        colors = cmap(np.linspace(0.2, 0.5, len(v)))\n",
    "    else:\n",
    "        thres = 2\n",
    "        colors = cmap(np.linspace(0.2, 0.8, len(v)))            \n",
    "    plot_stack(k, v, colors, thres)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot F\n",
    "total_count = total_count.sort_values(ascending=False)  # sort by count\n",
    "total_pc = total_count/sum(total_count)*100  # get percentage from count\n",
    "colors = [plt.cm.YlGn(0.6), '0.8']\n",
    "curr_sum = 0  # percentage sum (required for stacking bars)\n",
    "for i, pc in enumerate(total_pc):\n",
    "    ax['F'].barh('x', pc, label=total_pc.index[i], left=curr_sum,\n",
    "                 color=colors[i], edgecolor='w', linewidth=0.5)\n",
    "    curr_sum = curr_sum + pc    \n",
    "    # make label\n",
    "    label = \"{} ({})\".format(total_count.index[i], total_count[i]).upper()  \n",
    "    label_xpos = curr_sum - (pc/2)\n",
    "    ax['F'].text(label_xpos, 0, label, ha=\"center\", va=\"center\", c='k')    \n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Formatting\n",
    "\n",
    "# Subplot A settings\n",
    "ax['A'].set_xlabel('Age (months)', size=12)\n",
    "ax['A'].set_ylabel('Number of files', size=12)\n",
    "ax['A'].set_ylim([-10,250])\n",
    "ax['A'].spines[['top', 'right']].set_visible(False)\n",
    "ax['A'].grid(axis='both', color='w')\n",
    "ax['A'].set_facecolor('0.9')\n",
    "\n",
    "# Subplots B-E settings\n",
    "for i in ['B','C','D','E']:\n",
    "    ax[i].get_xaxis().set_visible(False)\n",
    "    ax[i].set_ylabel('Percentage total, %', size=12)\n",
    "    ax[i].set_ylim([0,100])\n",
    "    ax[i].spines[['top','bottom', 'right']].set_visible(False)\n",
    "    if i in ['B','D','E']:\n",
    "        ax[i].get_yaxis().set_visible(False)\n",
    "        ax[i].get_yaxis().set_visible(False)\n",
    "        ax[i].spines['left'].set_visible(False)        \n",
    "ax['D'].text(0,-2,'*Categories <2% not shown', ha=\"center\", va=\"top\", size=12)\n",
    "\n",
    "# Subplot F settings\n",
    "ax['F'].get_yaxis().set_visible(False)\n",
    "ax['F'].set_xlabel('Percentage total, %', size=12)\n",
    "ax['F'].set_xlim([0,100])\n",
    "ax['F'].spines[:].set_visible(False)\n",
    "\n",
    "# Set subplot titles\n",
    "ax['A'].set_title('\\nA. Age\\n', loc='left', size=15)\n",
    "ax['B'].set_title('\\nB. Sex\\n', loc='left', size=15)\n",
    "ax['C'].set_title('\\nC. Socioeconomic status\\n', loc='left', size=15)\n",
    "ax['D'].set_title('\\nD. Mother\\'s education*\\n', loc='left', size=15)\n",
    "ax['E'].set_title('\\nE. Group\\n', loc = 'left', size=15)\n",
    "ax['F'].set_title('\\nF. Total number of files\\n', loc='left', size=15)\n",
    "\n",
    "# Set figure title\n",
    "fig.suptitle('Demographics of child participants after data preprocessing\\n(Note: some children have multiple files)', fontsize=18)\n",
    "\n",
    "# Save figure\n",
    "# plt.savefig('../reports/images/child_demgph_processed.svg', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3bc6c-269a-4dbe-a353-c42715eaf35e",
   "metadata": {},
   "source": [
    "# 7. Save the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a34e50-5d03-4f7a-ad05-6a36d12f7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned 'data_idx' as 'data/data_idx_cleaned.pkl'\n",
    "with open('data/data_idx_cleaned.pkl', 'wb') as f:\n",
    "    pickle.dump(data_idx, f, -1)    \n",
    "print(\"Cleaned 'data_idx' was saved as 'data/data_idx_cleaned.pkl'. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f82c4-99a6-4ce4-a2f8-8d677f116f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to read data into readers and export as pickle\n",
    "\n",
    "# corpus_readers = [pylangacq.Reader.from_dir(d) for d in tqdm(paths['local_path'])]\n",
    "# with open('data/corpus_readers.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus_readers, f, -1)    \n",
    "# print(\"'corpus_readers' was saved as 'data/corpus_readers.pkl'. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eed396-3f51-4bd3-be08-1b3af2511f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
